{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks\n",
    "\n",
    "Training deep networks can entail a wide range of problems, from overfitting to vanishing/exploding gradients. This latter issue is especially troublesome as networks are trained by gradient descent, where the gradient is automatically computed through the backpropagation algorithm. If the gradient gets smaller and smaller when going backwards to the network then weights will not be updated and the network will never converge. Instead, in the opposite case, if the gradient gets bigget and bigger then weights updates will explode and the network will diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights initialization\n",
    "Keras defaults to **Glorot Initialization** with uniform distribution, where the boundaries of the distribution depend on the number of weights in the input and output layers. It is also possible to use the same initialization with a normal distribution. This type of initialization works best with linear layers and S-shaped activation functions like TanH, Sigmoid and Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd093327550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, kernel_initializer = 'glorot_normal')\n",
    "Dense(10, kernel_initializer = 'glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RELU and RELU-derived activation functions it is often better to use **He Initialization**, which differs for the scale of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd093327c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, kernel_initializer = 'he_normal')\n",
    "Dense(10, kernel_initializer = 'he_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally with SELU activation the preferred option is **LeCun initialization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd093290110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, kernel_initializer = 'lecun_normal')\n",
    "Dense(10, kernel_initializer = 'lecun_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "In the past the most common activation function was the S-shaped **sigmoid**. This can be a poor choice in many scenarios because of the vanishing gradient problem: when the sigmoid function saturates for values close to 0 or 1, the gradient is close to zero, making convergence very slow or even impossible. The **tanh** function also suffers from the same problem, but less because its non-saturation range is wider (from -1 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd093293050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, activation='sigmoid')\n",
    "Dense(10, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ReLU** function is the most common type of non-saturating activation function, both because it proved to work very well and because it is blazingly fast to compute. Its main drawback is that neurons using it could *die*: when the weighted sum of the inputs is negative for all instances in the training set, a ReLU adopting neuron will always output zero, making gradient descent unable to affect it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd093293510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several improvements have been proposed to solve this issue. **Leaky ReLU** ensures that even for negative values, the function never becomes flat (and thus the gradient never becomes null). A very small slope ($\\alpha$) is sufficient to ensure that neurons have a positive chance to wake-up. Additionally, the slope $\\alpha$ could also be randomly drawn during training (RReLU) or be an additional parameter that should be learned during training (PReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.advanced_activations.LeakyReLU at 0x7fd093293c10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10)\n",
    "keras.layers.LeakyReLU(alpha=0.1) # default alpha value is 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interesting variant is the **Exponential Linear Unit (ELU)**. which substitutes the flat part on the left with an exponential function. Thus, ELU can take negative values and has a non-zero gradient everywhere. Furthermore, it is smooth, meaning that it doesn't have have the typical kink of ReLU functions. While ELU is slower to compute than ReLU, it often makes convergence faster because of its nice properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd09329d050>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, activation='elu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, the award of best activation function, when its application is feasible, goes to the **Scaled ELU (SELU)** function. A sequential network, composed solely of dense layers, with each hidden layer using the SELU activation function is guaranteed to **self-normalize** under certain conditions. This is a very nice property because in a normalized network the output of every layer will preserve a zero mean and a unitary standard deviation, solving the vanishing/exploding gradient problem.\n",
    "* Inputs should be standardized (not min-max scaled!)\n",
    "* Hidden layers should use LeCun initialization with normal distribution\n",
    "* Network should be sequential\n",
    "* All layers should be dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7fd09329d7d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, activation='selu', kernel_initializer = 'lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpy36",
   "language": "python",
   "name": "newpy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
