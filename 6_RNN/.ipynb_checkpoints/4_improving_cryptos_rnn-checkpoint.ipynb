{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting cryptocurrencies using RNNs\n",
    "The problem can be approached from two different perspectives:\n",
    "* Classification (e.g. buy, hold, sell)\n",
    "* Regression (future price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Lorenzo-Giardi/tf-keras/blob/master/6_RNN/3_cryptocurrencies_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, BatchNormalization, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60 # last minutes to use as feature\n",
    "FUTURE_PERIOD_PREDICT = 3 # period over which to make the prediction\n",
    "RATIO_TO_PREDICT = \"ETH-USD\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f'{RATIO_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time        low       high       open      close      volume\n",
      "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
      "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
      "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
      "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
      "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
     ]
    }
   ],
   "source": [
    "col_names = ['time', 'low', 'high', 'open', 'close', 'volume']\n",
    "df = pd.read_csv(\"crypto_data/LTC-USD.csv\", names = col_names)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
      "\n",
      "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \n",
      "time                                                                      \n",
      "1528968660            NaN             NaN     871.719971        5.675361  \n",
      "1528968720      486.01001       26.019083     870.859985       26.856577  \n",
      "1528968780      486.00000        8.449400     870.099976        1.124300  \n",
      "1528968840      485.75000       26.994646     870.789978        1.749862  \n",
      "1528968900      486.00000       77.355759     870.000000        1.680500  \n"
     ]
    }
   ],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "ratios = ['BTC-USD', 'LTC-USD', 'ETH-USD', 'BCH-USD']\n",
    "for ratio in ratios:\n",
    "    # read CSV from path\n",
    "    df_path = f'crypto_data/{ratio}.csv'\n",
    "    df = pd.read_csv(df_path, names = col_names)\n",
    "    \n",
    "    # rename columns\n",
    "    df.rename(columns = {'close': f'{ratio}_close', 'volume': f'{ratio}_volume'}, inplace = True)\n",
    "    # set time as index\n",
    "    df.set_index('time', inplace = True)\n",
    "    # select only close price and volume columns\n",
    "    df = df[[f'{ratio}_close', f'{ratio}_volume']]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary classification: 1 (buy) vs 0 (hold/sell)\n",
    "def classify(current, future):\n",
    "    if float(future) > float (current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ETH-USD_close     future\n",
      "time                                \n",
      "1528968660            NaN  485.75000\n",
      "1528968720      486.01001  486.00000\n",
      "1528968780      486.00000  486.00000\n",
      "1528968840      485.75000  485.98999\n",
      "1528968900      486.00000  485.98999\n"
     ]
    }
   ],
   "source": [
    "# Define a new column with future price\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "print(main_df[[f'{RATIO_TO_PREDICT}_close', 'future']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ETH-USD_close     future  target\n",
      "time                                        \n",
      "1528968660            NaN  485.75000       0\n",
      "1528968720      486.01001  486.00000       0\n",
      "1528968780      486.00000  486.00000       0\n",
      "1528968840      485.75000  485.98999       1\n",
      "1528968900      486.00000  485.98999       0\n"
     ]
    }
   ],
   "source": [
    "# Transform future price into a binary target\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "print(main_df[[f'{RATIO_TO_PREDICT}_close', 'future', 'target']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split, normalization, sequence creation and data balancing\n",
    "\n",
    "Notice that since sequences are very close together and are highly correlated, it would be a bad idea to take a random sample to use as a validation/test set, as it would be extremely similar to instances in the training set. Instead, we have to take a whole period (possibly the most recent one) and use it for testing.\n",
    "\n",
    "In this case we'll take the last 5-10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time threshold for train-validation split: 1534922100\n"
     ]
    }
   ],
   "source": [
    "# ensure that time is sorted\n",
    "times = sorted(main_df.index.values)\n",
    "threshold = times[-int(0.05*len(times))]\n",
    "print(f'Time threshold for train-validation split: {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tain-validation split\n",
    "validation_main_df = main_df[(main_df.index >= threshold)]\n",
    "train_main_df = main_df[(main_df.index < threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a preprocessing function that will be applied to both, the training and validation sets.\n",
    "* Transform absolute prices into percentage changes\n",
    "* Normalize data to be in (0,1)\n",
    "* Drop NAs\n",
    "* Create sequences that will be used as features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a preprocessing function\n",
    "def preprocess_df(df, print_df = False):\n",
    "    df = df.drop('future', axis = 1)\n",
    "    \n",
    "    # transform data to percentage variation and normalize\n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace = True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "            \n",
    "    df.dropna(inplace = True)\n",
    "    if print_df: print(df[[f'{RATIO_TO_PREDICT}_close', 'target']].head())\n",
    "    \n",
    "    # create sequences of lenght SEQ_LEN to be used as features (X)\n",
    "    # sequential_data is a list [sequences(X), targets(y)]\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen = SEQ_LEN)\n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "            \n",
    "    random.shuffle(sequential_data)\n",
    "    if print_df: print('\\nsequential_data shape: ', np.shape(sequential_data))\n",
    "    \n",
    "    # Balance dataframe\n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    if print_df: print('\\nbuys_data shape: ', np.shape(buys))\n",
    "    if print_df: print('sells_data shape: ', np.shape(sells))\n",
    "    \n",
    "    lower = min(len(buys), len(sells))\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    if print_df: print('\\nsequential_data shape after balancing: ', np.shape(sequential_data))\n",
    "    \n",
    "    # split into X and y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ETH-USD_close  target\n",
      "time                             \n",
      "1528969140       0.004601       1\n",
      "1528969200       0.004601       1\n",
      "1528969260       0.021273       0\n",
      "1528969320       0.004601       0\n",
      "1528969380       0.004601       0\n",
      "\n",
      "sequential_data shape:  (82178, 2)\n",
      "\n",
      "buys_data shape:  (37098, 2)\n",
      "sells_data shape:  (45080, 2)\n",
      "\n",
      "sequential_data shape after balancing:  (74196, 2)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = preprocess_df(train_main_df, print_df = True)\n",
    "valid_x, valid_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Flatten(input_shape = (train_x.shape[1:])),\n",
    "        Dense(2, activation = 'softmax')])\n",
    "model.compile(loss = sparse_categorical_crossentropy, optimizer = 'nadam')\n",
    "print('\\n \\n *** Training linear model: *** \\n')\n",
    "\n",
    "history_bsl = model.fit(train_x, train_y,\n",
    "                   batch_size = BATCH_SIZE,\n",
    "                   epochs = EPOCHS,\n",
    "                   validation_data = (valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 60, 128)           52992     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 60, 128)           512       \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 60, 128)           99072     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 60, 128)           512       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 256,866\n",
      "Trainable params: 256,098\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    GRU(128, activation = 'relu', input_shape = (train_x.shape[1:]), return_sequences = True),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    GRU(128, activation = 'relu', return_sequences = True),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    GRU(128, activation = 'relu', return_sequences = False),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(2, activation = 'softmax'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay = 1e-6)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'],\n",
    "             )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = f'logs/{NAME}')\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_accuracy:.3f}.hdf5\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(f\"models/{NAME}/{filepath}\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max') # saves only the best ones\n",
    "\n",
    "# create folder for checkpoints (or it will throw an error if not found!)\n",
    "try:\n",
    "    os.makedirs(f\"models/{NAME}\")\n",
    "except:\n",
    "    print(\"Directory already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74196 samples, validate on 3260 samples\n",
      "Epoch 1/10\n",
      "74176/74196 [============================>.] - ETA: 0s - loss: 0.7195 - accuracy: 0.5210\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53466, saving model to models/ETH-USD-60-SEQ-3-PRED-1585046350/RNN_Final-01-0.535.hdf5\n",
      "74196/74196 [==============================] - 242s 3ms/sample - loss: 0.7195 - accuracy: 0.5211 - val_loss: 0.6905 - val_accuracy: 0.5347\n",
      "Epoch 2/10\n",
      "74176/74196 [============================>.] - ETA: 0s - loss: 0.6888 - accuracy: 0.5390\n",
      "Epoch 00002: val_accuracy improved from 0.53466 to 0.54049, saving model to models/ETH-USD-60-SEQ-3-PRED-1585046350/RNN_Final-02-0.540.hdf5\n",
      "74196/74196 [==============================] - 230s 3ms/sample - loss: 0.6888 - accuracy: 0.5390 - val_loss: 0.6886 - val_accuracy: 0.5405\n",
      "Epoch 3/10\n",
      "74176/74196 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.5436\n",
      "Epoch 00003: val_accuracy improved from 0.54049 to 0.54540, saving model to models/ETH-USD-60-SEQ-3-PRED-1585046350/RNN_Final-03-0.545.hdf5\n",
      "74196/74196 [==============================] - 239s 3ms/sample - loss: 0.6872 - accuracy: 0.5437 - val_loss: 0.6884 - val_accuracy: 0.5454\n",
      "Epoch 4/10\n",
      "74176/74196 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.5516\n",
      "Epoch 00004: val_accuracy improved from 0.54540 to 0.55859, saving model to models/ETH-USD-60-SEQ-3-PRED-1585046350/RNN_Final-04-0.559.hdf5\n",
      "74196/74196 [==============================] - 220s 3ms/sample - loss: 0.6852 - accuracy: 0.5516 - val_loss: 0.6876 - val_accuracy: 0.5586\n",
      "Epoch 5/10\n",
      " 3776/74196 [>.............................] - ETA: 3:15 - loss: 0.6867 - accuracy: 0.5352"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y,\n",
    "                   batch_size = BATCH_SIZE,\n",
    "                   epochs = EPOCHS,\n",
    "                   validation_data = (valid_x, valid_y),\n",
    "                   callbacks = [tensorboard, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpy36",
   "language": "python",
   "name": "newpy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
